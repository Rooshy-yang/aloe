<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>ALOE: Action-Level Off-Policy Evaluation for VLA Post-Training</title>
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=DM+Serif+Display:ital@0;1&family=Source+Sans+3:wght@400;500;600;700&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="style.css" />
</head>
<body>
  <header class="site-header">
    <div class="container">
      <span class="venue">ALOE</span>
    </div>
  </header>

  <main class="container">
    <h1 class="paper-title">ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training</h1>
    <p class="paper-subtitle">Real-world actorâ€“critic RL for VLA with off-policy value estimation under human-in-the-loop data</p>

    <ul class="authors">
      <li><a href="#" target="_blank" rel="noopener">Rushuai Yang</a><sup>1,2</sup><span class="equal">*</span></li>
      <li><a href="#" target="_blank" rel="noopener">Hecheng Wang</a><sup>1,3</sup><span class="equal">*</span></li>
      <li><a href="#" target="_blank" rel="noopener">Chiming Liu</a><sup>1</sup><span class="equal">*â€ </span></li>
      <li><a href="#" target="_blank" rel="noopener">Xiaohan Yan</a><sup>1</sup></li>
      <li><a href="#" target="_blank" rel="noopener">Yunlong Wang</a><sup>1</sup></li>
      <li><a href="#" target="_blank" rel="noopener">Xuan Du</a><sup>1</sup></li>
      <li><a href="#" target="_blank" rel="noopener">Shuoyu Yue</a><sup>1</sup></li>
      <li><a href="#" target="_blank" rel="noopener">Yongcheng Liu</a><sup>1</sup></li>
      <li><a href="#" target="_blank" rel="noopener">Chuheng Zhang</a><sup>4</sup></li>
      <li><a href="#" target="_blank" rel="noopener">Lizhe Qi</a><sup>3</sup></li>
      <li><a href="#" target="_blank" rel="noopener">Yi Chen</a><sup>2</sup></li>
      <li><a href="#" target="_blank" rel="noopener">Wei Shan</a><sup>1</sup></li>
      <li><a href="#" target="_blank" rel="noopener">Maoqing Yao</a><sup>1</sup></li>
    </ul>
    <p class="affiliations affiliations--compact"><sup>1</sup> AgiBot &nbsp; <sup>2</sup> The Hong Kong University of Science and Technology &nbsp; <sup>3</sup> Fudan University &nbsp; <sup>4</sup> Independent Researcher &nbsp; <span class="equal">*</span> Equal contribution &nbsp; <span class="equal">â€ </span> Corresponding author</p>

    <div class="action-links">
      <a href="#" class="btn btn-primary">ðŸ“„ Paper (PDF)</a>
      <a href="#" class="btn btn-secondary">ðŸ’» Code</a>
      <a href="#" class="btn btn-secondary">ðŸŽ¬ Video</a>
    </div>

    <section class="abstract">
      <h2>Abstract</h2>
      <p>
        We study how to improve large foundation vision-language-action (VLA) systems through online reinforcement learning (RL) in real-world settings. Central to this process is the value function, which provides learning signals to guide VLA learning from experience. In practice, the value function is estimated from trajectory fragments collected from different data sources, including historical policies and intermittent human interventions. Estimating the value function of current behavior quality from the mixture data is inherently an off-policy evaluation problem. However, prior work often adopts conservative on-policy estimation for stability, which avoids direct evaluation of the current high-capacity policy and limits learning effectiveness. In this paper, we propose <strong>ALOE</strong>, an action-level off-policy evaluation framework for VLA post-training. ALOE applies chunking-based temporal-difference bootstrapping to evaluate individual action sequences instead of predicting final task outcomes. This design improves effective credit assignment to critical action chunks under sparse rewards and supports stable policy improvement. We evaluate our method on three real-world manipulation tasks, including smartphone packing as a high-precision task, laundry folding as a long-horizon deformable-object task, and bimanual pick-and-place involving multi-object perception. Across all tasks, ALOE improves learning efficiency without compromising execution speed, showing that off-policy RL can be reintroduced in a reliable manner for real-world VLA post-training.
      </p>
    </section>

    <section class="method">
      <h2>Method</h2>
      <p>
        We adopt a real-world actorâ€“critic framework: the actor is a flow-matching foundation VLA and the critic is a lightweight ensemble Q-network. The actor outputs action sequences for online rollouts; the critic evaluates action chunks via Q-chunking temporal-difference updates. Training proceeds in three stages: (1) data collection under human intervention (warm-start with behavior cloning, then store policy and human-correction trajectories), (2) off-policy critic estimation on the aggregated replay buffer, and (3) policy improvement with pessimistic value estimation and advantage-weighted maximum likelihood.
      </p>
      <div class="hero-figure">
        <img src="images/teaser.png" alt="ALOE overview: actorâ€“critic framework for VLA post-training." width="960" height="400" loading="eager" onerror="this.src='https://placehold.co/960x400/1a1a2e/eee?text=ALOE+Teaser'; this.onerror=null;" />
      </div>
    </section>

    <section class="task-demos-and-results">
      <h2>Task Demos & Final Results</h2>
      <h3 class="subsection-title">Task Demos</h3>
      <p class="section-desc">We evaluate ALOE on three representative tasks designed to stress long-horizon reasoning, precise action selection, and robustness: <strong>Pack Smart Phone</strong> (align and attach phone case onto device body; precise pose alignment and recovery from misalignment), <strong>Folding Laundry</strong> (long-horizon deformable-object manipulation with grasp, flatten, and sequential folds), and <strong>Product Sorting</strong> (bimanual pick-place: identify, manipulate, and place objects from bins onto a conveyor belt).</p>
      <div class="media-row media-row-3">
        <figure>
          <video src="videos/phone_demo.mp4" controls muted loop playsinline autoplay></video>
        </figure>
        <figure>
          <video src="videos/cloth_demo.mp4" controls muted loop playsinline autoplay></video>
        </figure>
        <figure>
          <video src="videos/pick_object_demo.mp4" controls muted loop playsinline autoplay></video>
        </figure>
      </div>
      <h3 class="subsection-title">Final Results</h3>
      <p class="section-desc">Average success rate of the three manipulation tasks under real-world evaluation across multiple runs.</p>
      <div class="media-row media-row-3 final-results">
        <figure>
          <object data="images/phone_final_result.pdf" type="application/pdf" width="100%" height="360" title="Pack Smart Phone success rate">
            <p><a href="images/phone_final_result.pdf">Download PDF</a></p>
          </object>
        </figure>
        <figure>
          <object data="images/cloth_final_result.pdf" type="application/pdf" width="100%" height="360" title="Folding Laundry success rate">
            <p><a href="images/cloth_final_result.pdf">Download PDF</a></p>
          </object>
        </figure>
        <figure>
          <object data="images/object_final_result.pdf" type="application/pdf" width="100%" height="360" title="Product Sorting success rate">
            <p><a href="images/object_final_result.pdf">Download PDF</a></p>
          </object>
        </figure>
      </div>
    </section>

    <section class="continuous-operation">
      <h2>Continuous Operation</h2>
      <p class="section-desc">Pack Smart Phone is precision-critical: the phone case (17.8cm by 8.8cm) has minimal clearance inside the rigid container (17.5cm by 8.6cm). Under position control, slight pose errors can cause insertion failure, tipping, or damage. The video shows 21 consecutive trials with 19 successes.</p>
      <div class="media-row media-row-1">
        <figure>
          <video src="videos/phone_success_rate.mp4" controls muted loop playsinline autoplay></video>
        </figure>
      </div>
    </section>

    <section class="robustness">
      <h2>Robustness</h2>
      <p class="section-desc">We inject random perturbations during execution (e.g., to the garment in Folding Laundry or to the assembly in Pack Smart Phone) and measure whether the policy can re-adjust and still complete the task â€” evaluating recovery from unexpected disturbances.</p>
      <div class="media-row media-row-3">
        <figure>
          <video src="videos/phone_disturb_1.mp4" controls muted loop playsinline autoplay></video>
        </figure>
        <figure>
          <video src="videos/phone_disturb_2.mp4" controls muted loop playsinline autoplay></video>
        </figure>
        <figure>
          <video src="videos/phone_disturb_3.mp4" controls muted loop playsinline autoplay></video>
        </figure>
      </div>
      <div class="media-row media-row-1">
        <figure>
          <video src="videos/cloth_disturb.mp4" controls muted loop playsinline autoplay></video>
        </figure>
      </div>
    </section>

    <section class="q-value">
      <h2>Q-Value Learning</h2>
      <p class="section-desc">The learned action-value function identifies failure modes and successful recovery: the Q-value drops sharply when the robot fails a critical action (e.g., scooping the phone) and rises on recovery, providing finer credit assignment than trajectory-level estimation. The two figures below are shown in parallel: Q-value vs. task success (left) and additional Q(s,a) visualization (right).</p>
      <div class="media-row media-row-2">
        <figure>
          <img src="images/Q_value2_cropped.png" alt="Q-value and task success" loading="eager" />
        </figure>
        <figure>
          <img src="images/q_visualization_additional.png" alt="Q(s,a) visualization" loading="eager" />
        </figure>
      </div>
    </section>

    <section class="ood">
      <h2>Zero-Shot Generalization</h2>
      <p class="section-desc">We evaluate generalization on the Product Sorting task by replacing objects with ones never seen during training (different shapes and colors). The policy is evaluated without fine-tuning, assessing the VLAâ€™s ability to generalize to unseen object appearances and geometries.</p>
      <div class="media-row media-row-2">
        <figure>
          <video src="videos/pick_unseen_object_1.mp4" controls muted loop playsinline autoplay></video>
        </figure>
        <figure>
          <video src="videos/pick_useen_object_2.mp4" controls muted loop playsinline autoplay></video>
        </figure>
      </div>
    </section>

    <section class="highlights">
      <h2>Contributions</h2>
      <ul>
        <li>An off-policy policy evaluation framework for real-world VLA training under human-in-the-loop data collection, enabling action-level evaluation to support reliable policy improvement.</li>
        <li>Stabilization mechanisms for action-value learning in real-world settings, enabling reliable long-horizon credit assignment for flow-based VLA policies.</li>
        <li>Evaluation on three real-world manipulation tasks: improved task success, generalization, learning efficiency, and recovery from execution errors compared to prior VLA training methods.</li>
      </ul>
    </section>

    <section class="citation">
      <h2>Citation</h2>
      <pre class="bibtex"><code>@article{yang2026aloe,
  title         = {ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training},
  author        = {Yang, Rushuai and Wang, Hecheng and Liu, Chiming and Yan, Xiaohan and Wang, Yunlong and Du, Xuan and Yue, Shuoyu and Liu, Yongcheng and Zhang, Chuheng and Qi, Lizhe and Chen, Yi and Shan, Wei and Yao, Maoqing},
  year          = {2026},
  eprint        = {XXXX.XXXXX},
  archivePrefix = {arXiv},
  primaryClass  = {cs.RO}
}</code></pre>
    </section>

    <footer class="site-footer">
      <p>Project page for ALOE. Last updated: 2025.</p>
    </footer>
  </main>
  <script>
    document.querySelectorAll('video').forEach(function(v) { v.loop = true; });
  </script>
</body>
</html>

<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>ALOE: Action-Level Off-Policy Evaluation for VLA Post-Training</title>
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=DM+Serif+Display:ital@0;1&family=Source+Sans+3:wght@400;500;600;700&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="style.css" />
</head>
<body>
  <header class="site-header">
    <div class="container">
      <span class="venue">ALOE</span>
    </div>
  </header>

  <main class="container">
    <h1 class="paper-title">ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training</h1>
    <p class="paper-subtitle">Real-world actorâ€“critic RL for VLA with off-policy value estimation and human-in-the-loop data</p>

    <ul class="authors">
      <li><a href="#" target="_blank" rel="noopener">Rushuai Yang</a><sup>1,2</sup><span class="equal">*</span></li>
      <li><a href="#" target="_blank" rel="noopener">Hecheng Wang</a><sup>1,3</sup><span class="equal">*</span></li>
      <li><a href="#" target="_blank" rel="noopener">Chiming Liu</a><sup>1</sup><span class="equal">*â€ </span></li>
      <li><a href="#" target="_blank" rel="noopener">Xiaohan Yan</a><sup>1</sup></li>
      <li><a href="#" target="_blank" rel="noopener">Yunlong Wang</a><sup>1</sup></li>
      <li><a href="#" target="_blank" rel="noopener">Xuan Du</a><sup>1</sup></li>
      <li><a href="#" target="_blank" rel="noopener">Shuoyu Yue</a><sup>1</sup></li>
      <li><a href="#" target="_blank" rel="noopener">Yongcheng Liu</a><sup>1</sup></li>
      <li><a href="#" target="_blank" rel="noopener">Chuheng Zhang</a><sup>4</sup></li>
      <li><a href="#" target="_blank" rel="noopener">Lizhe Qi</a><sup>3</sup></li>
      <li><a href="#" target="_blank" rel="noopener">Yi Chen</a><sup>2</sup></li>
      <li><a href="#" target="_blank" rel="noopener">Wei Shan</a><sup>1</sup></li>
      <li><a href="#" target="_blank" rel="noopener">Maoqing Yao</a><sup>1</sup></li>
    </ul>
    <ul class="affiliations">
      <li><sup>1</sup> AgiBot</li>
      <li><sup>2</sup> The Hong Kong University of Science and Technology</li>
      <li><sup>3</sup> Fudan University</li>
      <li><sup>4</sup> Independent Researcher</li>
      <li class="note"><span class="equal">*</span> Equal contribution &nbsp; <span class="equal">â€ </span> Corresponding author</li>
    </ul>

    <div class="action-links">
      <a href="#" class="btn btn-primary">ğŸ“„ Paper (PDF)</a>
      <a href="#" class="btn btn-secondary">ğŸ’» Code</a>
      <a href="#" class="btn btn-secondary">ğŸ¬ Video</a>
    </div>

    <section class="abstract">
      <h2>Abstract</h2>
      <p>
        We study how to improve large foundation vision-language-action (VLA) systems through online reinforcement learning (RL) in real-world settings. Central to this process is the value function, which provides learning signals to guide VLA learning from experience. In practice, the value function is estimated from trajectory fragments collected from different data sources, including historical policies and intermittent human interventions. Estimating the value function of current behavior quality from the mixture data is inherently an off-policy evaluation problem. However, prior work often adopts conservative on-policy estimation for stability, which avoids direct evaluation of the current high-capacity policy and limits learning effectiveness.
      </p>
      <p>
        In this paper, we propose <strong>ALOE</strong>, an action-level off-policy evaluation framework for VLA post-training. ALOE applies chunking-based temporal-difference bootstrapping to evaluate individual action sequences instead of predicting final task outcomes. This design improves effective credit assignment to critical action chunks under sparse rewards and supports stable policy improvement. We evaluate our method on three real-world manipulation tasks, including smartphone packing as a high-precision task, laundry folding as a long-horizon deformable-object task, and bimanual pick-and-place involving multi-object perception. Across all tasks, ALOE improves learning efficiency without compromising execution speed, showing that off-policy RL can be reintroduced in a reliable manner for real-world VLA post-training.
      </p>
    </section>

    <section class="method">
      <h2>Method</h2>
      <p>
        We adopt a real-world actorâ€“critic framework: the actor is a flow-matching foundation VLA and the critic is a lightweight ensemble Q-network. The actor outputs action sequences for online rollouts; the critic evaluates action chunks via Q-chunking temporal-difference updates. Training proceeds in three stages: (1) data collection under human intervention (warm-start with behavior cloning, then store policy and human-correction trajectories), (2) off-policy critic estimation on the aggregated replay buffer, and (3) policy improvement with pessimistic value estimation and advantage-weighted maximum likelihood.
      </p>
      <div class="hero-figure">
        <img src="images/teaser.png" alt="ALOE overview: actorâ€“critic framework for VLA post-training." width="960" height="400" onerror="this.src='https://placehold.co/960x400/1a1a2e/eee?text=ALOE+Teaser'; this.onerror=null;" />
        <p class="caption">Overview of our real-world actorâ€“critic framework for VLA post-training: actor (flow-matching VLA) + critic (ensemble Q-network), with data collection under human intervention, off-policy critic estimation, and policy improvement.</p>
      </div>
    </section>

    <section class="task-demos">
      <h2>Task Demos</h2>
      <p class="section-desc">Demonstrations on three real-world manipulation tasks: smartphone packing (high-precision), laundry folding (long-horizon deformable), and bimanual pick-and-place (multi-object perception).</p>
      <div class="media-row media-row-3">
        <figure>
          <video src="videos/phone_demo.mp4" controls muted loop playsinline></video>
          <figcaption>Smartphone packing</figcaption>
        </figure>
        <figure>
          <video src="videos/cloth_demo.mp4" controls muted loop playsinline></video>
          <figcaption>Laundry folding</figcaption>
        </figure>
        <figure>
          <video src="videos/pick_object_demo.mp4" controls muted loop playsinline></video>
          <figcaption>Bimanual pick-and-place</figcaption>
        </figure>
      </div>
    </section>

    <section class="continuous-operation">
      <h2>Continuous Operation</h2>
      <p class="section-desc">åœ¨æ‰‹æœºè£…ç›’ä»»åŠ¡ä¸­ï¼ŒåŒ…è£…åº•ç›’ä¸æ‰‹æœºä¹‹é—´çš„å°ºå¯¸ä½™é‡ä»…æœ‰ 1ï½2 æ¯«ç±³ï¼Œå‡ ä¹æ²¡æœ‰å¤šä½™ç©ºé—´ï¼Œè€Œä¸”ç›’ä½“ä¸ºç¡¬è´¨çº¸å£³ç»“æ„ï¼Œç¼ºä¹å½¢å˜ç¼“å†²ã€‚åœ¨ä½ç½®æ§åˆ¶ä¸‹ï¼Œä¸€æ—¦ä½å§¿å‡ºç°è½»å¾®åå·®ï¼Œå®¹æ˜“å¯¼è‡´æ’å…¥å¤±è´¥ã€æ‰“ç¿»ç”šè‡³æŸåç‰©æ–™ã€‚ä¸‹æ–¹è§†é¢‘ä¸ºè¿ç»­ 21 æ¬¡æµ‹è¯•æ•ˆæœï¼Œä»…å¤±è´¥ 2 æ¬¡ã€‚</p>
      <div class="media-row media-row-1">
        <figure>
          <video src="videos/phone_success_rate.mp4" controls muted loop playsinline></video>
          <figcaption>Smartphone packing: 21 trials, 2 failures (continuous operation)</figcaption>
        </figure>
      </div>
    </section>

    <section class="final-results">
      <h2>Final Results</h2>
      <p class="section-desc">Task success and learning curves across the three tasks (1Ã—3).</p>
      <div class="media-row media-row-3">
        <figure>
          <img src="images/phone_final_result.png" alt="Phone packing final result" />
          <figcaption>Smartphone packing</figcaption>
        </figure>
        <figure>
          <img src="images/cloth_final_result.png" alt="Cloth folding final result" />
          <figcaption>Laundry folding</figcaption>
        </figure>
        <figure>
          <img src="images/object_final_result.png" alt="Pick-and-place final result" />
          <figcaption>Bimanual pick-and-place</figcaption>
        </figure>
      </div>
    </section>

    <section class="robustness">
      <h2>Robustness</h2>
      <p class="section-desc">Recovery under external disturbances: phone case assembly with disturbance (Ã—3), then cloth folding with disturbance.</p>
      <div class="media-row media-row-3">
        <figure>
          <video src="videos/phone_disturb_1.mp4" controls muted loop playsinline></video>
          <figcaption>Phone disturb 1</figcaption>
        </figure>
        <figure>
          <video src="videos/phone_disturb_2.mp4" controls muted loop playsinline></video>
          <figcaption>Phone disturb 2</figcaption>
        </figure>
        <figure>
          <video src="videos/phone_disturb_3.mp4" controls muted loop playsinline></video>
          <figcaption>Phone disturb 3</figcaption>
        </figure>
      </div>
      <div class="media-row media-row-1">
        <figure>
          <video src="videos/cloth_disturb.mp4" controls muted loop playsinline></video>
          <figcaption>Cloth disturb</figcaption>
        </figure>
      </div>
    </section>

    <section class="q-value">
      <h2>Q-Value Learning</h2>
      <p class="section-desc">Critic Q-value curve and correlation with task success.</p>
      <div class="hero-figure">
        <img src="images/Q_value2_cropped.png" alt="Q-value vs success" />
        <p class="caption">Q-value curve (Q_value2_cropped).</p>
      </div>
    </section>

    <section class="ood">
      <h2>OOD Generalization</h2>
      <p class="section-desc">Zero-shot generalization to unseen objects: pick unseen object 1 and 2.</p>
      <div class="media-row media-row-2">
        <figure>
          <video src="videos/pick_unseen_object_1.mp4" controls muted loop playsinline></video>
          <figcaption>Pick unseen object 1</figcaption>
        </figure>
        <figure>
          <video src="videos/pick_useen_object_2.mp4" controls muted loop playsinline></video>
          <figcaption>Pick unseen object 2</figcaption>
        </figure>
      </div>
    </section>

    <section class="highlights">
      <h2>Contributions</h2>
      <ul>
        <li>An off-policy policy evaluation framework for real-world VLA training under human-in-the-loop data collection, enabling action-level evaluation to support reliable policy improvement.</li>
        <li>Stabilization mechanisms for action-value learning in real-world settings, enabling reliable long-horizon credit assignment for flow-based VLA policies.</li>
        <li>Evaluation on three real-world manipulation tasks: improved task success, generalization, learning efficiency, and recovery from execution errors compared to prior VLA training methods.</li>
      </ul>
    </section>

    <section class="citation">
      <h2>Citation</h2>
      <pre class="bibtex"><code>@article{yang2026aloe,
  title   = {ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training},
  author  = {Yang, Rushuai and Wang, Hecheng and Liu, Chiming and Yan, Xiaohan and Wang, Yunlong and Du, Xuan and Yue, Shuoyu and Liu, Yongcheng and Zhang, Chuheng and Qi, Lizhe and Chen, Yi and Shan, Wei and Yao, Maoqing},
  journal = {arXiv preprint},
  year    = {2026}
}</code></pre>
    </section>

    <footer class="site-footer">
      <p>Project page for ALOE. Last updated: 2025.</p>
    </footer>
  </main>
</body>
</html>
